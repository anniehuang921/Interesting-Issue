{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用不同的深度，比較決策樹結果  \n",
    "https://machine-learning-python.kspax.io/decision_trees/ex1_decision_tree_regression\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 造數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\n",
    "\n",
    "y = np.sin(X).ravel()##ravel() 連續輸出的一為矩陣\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[::5] += 3 * (0.5 - rng.rand(16)) ## 每五筆加入一個雜訊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立 Decision Tree 迴歸模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_1 = DecisionTreeRegressor(max_depth = 2) ## 最大深度為 2 的決策數\n",
    "regr_2 = DecisionTreeRegressor(max_depth = 5) ## 最大深度為 5 的決策數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_1.fit(X,y)\n",
    "regr_2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(0.0,5.0,0.01)[:,np.newaxis] \n",
    "## np.arrange(起始點, 結束點, 間隔)：np.arange(0.0, 5.0, 0.01)在0~5之間每0.01取一格，建立預測輸入點矩陣\n",
    "## np.newaxis：增加矩陣維度\n",
    "y_1= regr_1.predict(X_test)\n",
    "y_2= regr_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeRegressor in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeRegressor(sklearn.base.RegressorMixin, BaseDecisionTree)\n",
      " |  DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree regressor.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion and minimizes the L2 loss\n",
      " |      using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
      " |      squared error with Friedman's improvement score for potential splits,\n",
      " |      and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
      " |      using the median of each terminal node.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=0)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  presort : deprecated, default='deprecated'\n",
      " |      This parameter is deprecated and will be removed in v0.24.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the\n",
      " |      (normalized) total reduction of the criterion brought\n",
      " |      by that feature. It is also known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier : A decision tree classifier.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_diabetes\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      " |  >>> X, y = load_diabetes(return_X_y=True)\n",
      " |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      " |  >>> cross_val_score(regressor, X, y, cv=10)\n",
      " |  ...                    # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      " |         0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      " |      Build a decision tree regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      " |          ``order='C'`` for maximum efficiency.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : array-like of shape (n_samples, n_features),             default=None\n",
      " |          The indexes of the sorted training input samples. If many tree\n",
      " |          are grown on the same dataset, this allows the ordering to be\n",
      " |          cached between trees. If None, the data will be sorted here.\n",
      " |          Don't use this parameter unless you know what to do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeRegressor\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  n_classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X,y, c='darkorange',label='data')## c:color\n",
    "#plt.scatter(X_,y_, c='red',label='data1')\n",
    "plt.plot(X_test,y_1,color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test,y_2,color=\"yellowgreen\",label=\"max_depth=5\",linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()##顯示圖例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_1_.fit(X_[:,np.newaxis] ,y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'import numpy as np\\nfrom sklearn.tree import DecisionTreeRegressor\\nimport matplotlib.pyplot as plt',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\ny = np.sin(X)#.ravel()##ravel() 連續輸出的一為矩陣',\n",
       " 'y',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\ny = np.sin(X).ravel()##ravel() 連續輸出的一為矩陣',\n",
       " 'y',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\ny = np.sin(X)#.ravel()##ravel() 連續輸出的一為矩陣',\n",
       " 'y',\n",
       " 'y.type()',\n",
       " 'y.shape',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\ny = np.sin(X).ravel()##ravel() 連續輸出的一為矩陣',\n",
       " 'y.shape',\n",
       " '1.shape',\n",
       " 'A= [1 2 3 4 5 6 7 8]',\n",
       " 'A =[1 2 3 4 5 6 7 8]',\n",
       " 'A =np.array([1 2 3 4 5 6 7 8])',\n",
       " 'A =np.array([1, 2, 3, 4, 5, 6, 7, 8])',\n",
       " 'A',\n",
       " 'A[:]',\n",
       " 'A[::]',\n",
       " 'A[::2]',\n",
       " 'A[::3]',\n",
       " 'rng.rand(3)',\n",
       " '5* rng.rand(6)',\n",
       " 'np.sort(5* rng.rand(6))',\n",
       " 'np.sin(np.sort(5* rng.rand(6)))',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\nX_ = np.sort(5* rng.rand(80),axis = 0)\\ny = np.sin(X).ravel()##ravel() 連續輸出的一為矩陣\\ny_ = np.sin(X)',\n",
       " 'y[::5] += 3 * (0.5 - rng.rand(16)) ## 每五筆加入一個雜訊',\n",
       " 'y_[::5] += 3 * (0.5 - rng.rand(16)) ',\n",
       " 'y_',\n",
       " 'rng = np.random.RandomState(1)\\nX = np.sort(5* rng.rand(80,1),axis = 0) ## 隨機產生80個數字，區間[0,5]\\nX_ = np.sort(5* rng.rand(80),axis = 0)\\ny = np.sin(X).ravel()##ravel() 連續輸出的一為矩陣\\ny_ = np.sin(X_)',\n",
       " 'y[::5] += 3 * (0.5 - rng.rand(16)) ## 每五筆加入一個雜訊',\n",
       " 'y_[::5] += 3 * (0.5 - rng.rand(16)) ',\n",
       " 'y_',\n",
       " 'regr_1 = DecisionTreeRegressor(max_depth = 2) ## 最大深度為 2 的決策數\\nregr_2 = DecisionTreeRegressor(max_depth = 5) ## 最大深度為 5 的決策數',\n",
       " 'regr_1.fit(X,y)\\nregr_2.fit(X,y)',\n",
       " 'X_test = np.arange(0.0,5.0,0.01)[:,np.newaxis]\\ny_1= regr_1.predict(X_test)\\ny_2= regr_2.predict(X_test)',\n",
       " 'regr_1_ = DecisionTreeRegressor(max_depth = 2) ## 最大深度為 2 的決策數\\nregr_2_ = DecisionTreeRegressor(max_depth = 5) ## 最大深度為 5 的決策數\\nregr_1_.fit(X_,y_)\\nregr_2_.fit(X_,y_)\\ny_1_= regr_1_.predict(X_test)\\ny_2_= regr_2_.predict(X_test)',\n",
       " 'X_.shape',\n",
       " 'X.shape',\n",
       " 'X',\n",
       " 'y_.shape',\n",
       " 'y.shape',\n",
       " 'help(DecisionTreeRegressor)',\n",
       " 'plt.figure()\\nplt.scatter(X,y, c=\\'darkorange\\',label=\\'data\\')## c:color\\nplt.plot(X_test,y_1,color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\\nplt.plot(X_test,y_2,color=\"yellowgreen\",label=\"max_depth=5\",linewidth=2)\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\nplt.title(\"Decision Tree Regression\")\\nplt.legend()##顯示圖例\\nplt.show()',\n",
       " 'plt.figure()\\nplt.scatter(X,y, c=\\'darkorange\\',label=\\'data\\')## c:color\\nplt.scatter(X_,y_, c=\\'red\\',label=\\'data1\\')\\nplt.plot(X_test,y_1,color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\\nplt.plot(X_test,y_2,color=\"yellowgreen\",label=\"max_depth=5\",linewidth=2)\\nplt.xlabel(\"data\")\\nplt.ylabel(\"target\")\\nplt.title(\"Decision Tree Regression\")\\nplt.legend()##顯示圖例\\nplt.show()',\n",
       " 'regr_1_.fit(X_,y_)',\n",
       " 'regr_1_.fit(X_[:,np.newaxis] ,y_)',\n",
       " 'In']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
